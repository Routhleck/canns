无监督学习
==========

.. warning::

   ⚠️ **重要提示**：本文档部分内容仍在开发和验证中，可能存在不完善之处。建议仅用于参考，重要项目前请与开发团队确认相关功能的完整性。



场景描述
--------

无监督学习是神经网络自组织学习的核心。本系列教程关注经典的生物启发学习算法，特别是主成分分析（PCA）的神经实现：

- **Oja 规则**：提取第一主成分
- **Sanger 规则**：提取多个正交主成分
- 与传统 PCA 算法的对比

你将学到
--------

1. Oja 规则的数学原理和实现
2. 如何从高维数据中提取主成分
3. Sanger 规则的正交化机制
4. 学习算法的收敛性分析
5. 与 sklearn PCA 的对比验证

教程列表
--------

.. toctree::
   :maxdepth: 1

   oja_pca
   sanger_orthogonal_pca
   algorithm_comparison

适用人群
--------

- 对生物启发学习算法感兴趣的研究人员
- 需要理解 PCA 神经实现的学生
- 研究无监督学习的开发者

前置知识
--------

- 线性代数（特征值、特征向量）
- 基本的统计学（方差、协方差）
- Python 和 NumPy

核心算法
--------

**Oja 规则**：

.. math::

   \Delta W = \eta \cdot (y \cdot x^T - y^2 \cdot W)

- 第一项：Hebbian 增强
- 第二项：权重归一化

**Sanger 规则（GHA）**：

.. math::

   \Delta W_{ij} = \eta \cdot y_i \cdot (x_j - \sum_{k=1}^{i} W_{kj} y_k)

- 通过 Gram-Schmidt 正交化提取多个主成分

实际应用
--------

- **降维**：高维数据的低维表示
- **特征提取**：识别数据的主要变化方向
- **数据压缩**：保留最重要的信息
- **去噪**：过滤噪声保留信号

理论意义
--------

Oja 和 Sanger 规则展示了复杂的统计操作（PCA）可以通过简单的局部学习规则涌现，这对理解生物神经网络的学习机制具有重要意义。

开始学习
--------

从 :doc:`oja_pca` 开始，学习神经网络如何自动发现数据结构！
